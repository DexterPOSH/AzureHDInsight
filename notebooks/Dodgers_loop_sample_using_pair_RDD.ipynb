{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "spark", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1515736974360_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-dexter.c2whhr1xdp4edkuukzeqbn2e0a.rx.internal.cloudapp.net:8088/proxy/application_1515736974360_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.13:30060/node/containerlogs/container_1515736974360_0005_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n<pyspark.sql.session.SparkSession object at 0x7fd0985a87d0>"}], "metadata": {"collapsed": false}}, {"source": "## Dodgers loop sensor example", "cell_type": "markdown", "metadata": {}}, {"source": "Download the dataset for the Dodgers traffic from the link https://archive.ics.uci.edu/ml/datasets/dodgers+loop+sensor\n\nFrom the above link we land to the pager -> https://archive.ics.uci.edu/ml/machine-learning-databases/event-detection/\nwhere we will download two files 'Dodgers.data\" & \"Dodgers.events\"", "cell_type": "markdown", "metadata": {}}, {"source": "### Dataset information \n\nCopied from the above website\n\nThis loop sensor data was collected for the Glendale on ramp for the 101 North freeway in Los Angeles. It is close enough to the stadium to see unusual traffic after a Dodgers game, but not so close and heavily used by game traffic so that the signal for the extra traffic is overly obvious. \n\nNOTE: This is an on ramp near the stadium so event traffic BEGINS at or near the END of the event time. \n\nThe observations were taken over 25 weeks, 288 time slices per day (5 minute count aggregates). \n\nThe goal is to predict the presence of a baseball game at Dodgers stadium\n\nAttribute Information:\n\n1. Date: MM/DD/YY \n2. Time: (H)H:MM (military time) \n3. Count: Number of cars measured for the previous five minutes \nRows: Each five minute time slice is represented by one row \n\nFor .events file: \n1. Date: MM/DD/YY \n2. Begin event time: HH:MM:SS (military) \n3. End event time: HH:MM:SS (military) \n4. Game attendance \n5. Away team \n6. W/L score\n\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "# Start with specifying where the files live in the Azure blob storage (default storage account mapped to the cluster)\ntrafficPath = 'wasb:///example/data/Dodgers.data'\ngamesPath = 'wasb:///example/data/Dodgers.events'", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 3, "cell_type": "code", "source": "# load and see the traffic dataset\ntraffic = sc.textFile(trafficPath)\ntraffic.take(5) # Each row represens a 5 minute slice of time and no of cars that passed in that time slice", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[u'4/10/2005 0:00,-1', u'4/10/2005 0:05,-1', u'4/10/2005 0:10,-1', u'4/10/2005 0:15,-1', u'4/10/2005 0:20,-1']"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "# Load the events file and take a look at it\ngames = sc.textFile(gamesPath)\ngames.take(10) # refer to the info on what it contains (Date,Start, End times, #spectators, Away team, WL score)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[u'04/12/05,13:10:00,16:23:00,55892,San Francisco,W 9-8\\ufffd', u'04/13/05,19:10:00,21:48:00,46514,San Francisco,W 4-1\\ufffd', u'04/15/05,19:40:00,21:48:00,51816,San Diego,W 4-0\\ufffd', u'04/16/05,19:10:00,21:52:00,54704,San Diego,W 8-3\\ufffd', u'04/17/05,13:10:00,15:31:00,53402,San Diego,W 6-0\\ufffd', u'04/25/05,19:10:00,21:33:00,36876,Arizona,L 4-2\\ufffd', u'04/26/05,19:10:00,22:00:00,44486,Arizona,L 3-2\\ufffd', u'04/27/05,19:10:00,22:17:00,54387,Arizona,L 6-3\\ufffd', u'04/29/05,19:40:00,22:01:00,40150,Colorado,W 6-3\\ufffd', u'04/30/05,19:10:00,21:45:00,54123,Colorado,W 6-2\\ufffd']"}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "from datetime import datetime\nimport csv\nfrom StringIO import StringIO\n\n# function to parse the traffic data \ndef parseTraffic(row):\n    DATE_FMT = \"%m/%d/%Y %H:%M\"\n    row = row.split(\",\")\n    row[0] = datetime.strptime(row[0], DATE_FMT) # parse the date to get the object\n    row[1] = int(row[1])\n    return (row[0], row[1])", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 7, "cell_type": "code", "source": "# create a pair RDD , create a python tuple (date, #cars)\ntrafficParsed = traffic.map(parseTraffic)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 8, "cell_type": "code", "source": "trafficParsed.first()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(datetime.datetime(2005, 4, 10, 0, 0), -1)"}], "metadata": {"collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "trafficParsed.values().take(500) # why there are -1 values? not sure :O", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 23, 42, 37, 24, 39, 25, 33, 34, 33, 34, 29, 33, 42, 41, 38, 38, 40, 30, 27, 42, 38, 39, 29, 31, 29, 31, 37, 19, 34, 26, 26, 26, 39, 29, 25, 32, 21, 26, 31, 35, 23, 39, 26, 32, 25, 23, 22, 27, 23, 19, 22, 23, 33, 35, 25, 21, 18, 28, 24, 32, 24, 13, 22, 29, 34, 22, 14, 29, 20, 19, 30, 28, 24, 26, 13, 15, 32, 30, 18, 27, 20, 29, 28, 33, 37, 38, 35, 35, 33, 31, 31, 41, 53, 39, 25, 42, 35, 40, 40, 38, 38, 48, 29, 33, 40, 38, 29, 36, 42, 25, 28, 32, 40, 41, 32, 37, 42, 39, 37, 38, 20]"}], "metadata": {"collapsed": false}}, {"source": "## Computing a daily trend", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "# How many cars are passing by the stadium?\n# Summarizing a pair RDD\ndailyTrend = trafficParsed.map(lambda x: (x[0].date(), x[1])).reduceByKey(lambda x,y: x+y)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 11, "cell_type": "code", "source": "dailyTrend.take(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(datetime.date(2005, 8, 9), 5958), (datetime.date(2005, 6, 29), 5437), (datetime.date(2005, 8, 17), 6673), (datetime.date(2005, 9, 6), 6402), (datetime.date(2005, 5, 22), 4977)]"}], "metadata": {"collapsed": false}}, {"execution_count": 14, "cell_type": "code", "source": "# Sort the data by values\ndailyTrend.sortBy(lambda x: -x[1]).take(10) # -x[1] specify for the descending order\n# Below shows the dates with highest amount of traffic", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(datetime.date(2005, 7, 28), 7661), (datetime.date(2005, 7, 29), 7499), (datetime.date(2005, 8, 12), 7287), (datetime.date(2005, 7, 27), 7238), (datetime.date(2005, 9, 23), 7175), (datetime.date(2005, 7, 26), 7163), (datetime.date(2005, 5, 20), 7119), (datetime.date(2005, 8, 11), 7110), (datetime.date(2005, 9, 8), 7107), (datetime.date(2005, 9, 7), 7082)]"}], "metadata": {"collapsed": false}}, {"source": "**It would be interesting to correlate the dates with highest no of traffic with the days there was a Dodgers game.**", "cell_type": "markdown", "metadata": {}}, {"source": " For the above we need to merge this data set with the games dataset\n \n Merging pair RDDs, similar to join operation. Spark allows this merging on the key.\n\n 3 types of join in spark - join, leftOuterJoin , rightOuterJoin\n- Join - returns a new pair RDD, values whose keys match are grouped together. Key dropped if not present in both RDDs.\n- leftOuterJoin - All keys from the left RDD are returned, matching keys in the right RDD are grouped.\n- rightOuterJoin - reverse of the above leftOuterJoin.\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "# Merging with the games data\n# First need to parse the games data to return a tuple\ndef parseGames(row):\n    DATE_FMT = \"%m/%d/%y\"\n    row = row.split(\",\")\n    row[0] = datetime.strptime(row[0], DATE_FMT).date()\n    return (row[0], row[4]) # date and opponent team tuple\n\ngamesParsed = games.map(parseGames)\ngamesParsed.take(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(datetime.date(2005, 4, 12), u'San Francisco'), (datetime.date(2005, 4, 13), u'San Francisco'), (datetime.date(2005, 4, 15), u'San Diego'), (datetime.date(2005, 4, 16), u'San Diego'), (datetime.date(2005, 4, 17), u'San Diego'), (datetime.date(2005, 4, 25), u'Arizona'), (datetime.date(2005, 4, 26), u'Arizona'), (datetime.date(2005, 4, 27), u'Arizona'), (datetime.date(2005, 4, 29), u'Colorado'), (datetime.date(2005, 4, 30), u'Colorado')]"}], "metadata": {"collapsed": false}}, {"execution_count": 20, "cell_type": "code", "source": "# join this with the original RDD leftOuterJoin, reason is we want to keep all the dates in our dataset\n# whether or not there was a game that day\ndailyTrendCombined = dailyTrend.leftOuterJoin(gamesParsed)\ndailyTrendCombined.take(10) # note that the second element is now a tuple with values from both the RDD", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(datetime.date(2005, 9, 24), (5848, u'Pittsburgh')), (datetime.date(2005, 8, 11), (7110, u'Philadelphia')), (datetime.date(2005, 6, 21), (5759, None)), (datetime.date(2005, 5, 24), (4138, None)), (datetime.date(2005, 6, 13), (5974, None)), (datetime.date(2005, 7, 18), (5994, None)), (datetime.date(2005, 4, 23), (5366, None)), (datetime.date(2005, 6, 29), (5437, u'San Diego')), (datetime.date(2005, 8, 15), (5329, None)), (datetime.date(2005, 6, 1), (6520, u'Chicago Cubs'))]"}], "metadata": {"collapsed": false}}, {"execution_count": 21, "cell_type": "code", "source": "# helper function to report game or regular day\ndef checkGameDay(row):\n    if row[1][1] == None: # if this is empty that means no game\n        return (row[0], row[1][1], \"Regular Day\", row[1][0])\n    else:\n        return (row[0], row[1][1], \"Game Day\", row[1][0])\n    \n# Apply the above function to the trend combined RDD\ndailyTrendbyGames = dailyTrendCombined.map(checkGameDay)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 22, "cell_type": "code", "source": "dailyTrendbyGames.take(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(datetime.date(2005, 9, 24), u'Pittsburgh', 'Game Day', 5848), (datetime.date(2005, 8, 11), u'Philadelphia', 'Game Day', 7110), (datetime.date(2005, 6, 21), None, 'Regular Day', 5759), (datetime.date(2005, 5, 24), None, 'Regular Day', 4138), (datetime.date(2005, 6, 13), None, 'Regular Day', 5974)]"}], "metadata": {"collapsed": false}}, {"execution_count": 23, "cell_type": "code", "source": "dailyTrendbyGames.sortBy(lambda x:-x[3]).take(10) # now we sort based on the decreasing car count and get top 10 records", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(datetime.date(2005, 7, 28), u'Cincinnati', 'Game Day', 7661), (datetime.date(2005, 7, 29), u'St. Louis', 'Game Day', 7499), (datetime.date(2005, 8, 12), u'NY Mets', 'Game Day', 7287), (datetime.date(2005, 7, 27), u'Cincinnati', 'Game Day', 7238), (datetime.date(2005, 9, 23), u'Pittsburgh', 'Game Day', 7175), (datetime.date(2005, 7, 26), u'Cincinnati', 'Game Day', 7163), (datetime.date(2005, 5, 20), u'LA Angels', 'Game Day', 7119), (datetime.date(2005, 8, 11), u'Philadelphia', 'Game Day', 7110), (datetime.date(2005, 9, 8), None, 'Regular Day', 7107), (datetime.date(2005, 9, 7), u'San Francisco', 'Game Day', 7082)]"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}