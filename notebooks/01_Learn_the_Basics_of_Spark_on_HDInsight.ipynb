{"nbformat_minor": 0, "cells": [{"source": "# Basics of Spark on HDInsight\n\n<a href=\"http://spark.apache.org/\" target=\"_blank\">Apache Spark</a> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. When you provision a Spark cluster in HDInsight, you provision Azure compute resources with Spark installed and configured. The data to be processed is stored in Azure Blob storage (WASB).\n\n![Spark on HDInsight](https://mysstorage.blob.core.windows.net/notebookimages/overview/SparkArchitecture.png \"Spark on HDInsight\")", "cell_type": "markdown", "metadata": {}}, {"source": "Now that you have created a Spark cluster, let us understand some basics of working with Spark on HDInsight. For detailed discussion on working with Spark, see [Spark Programming Guide](http://spark.apache.org/docs/2.0.0/sql-programming-guide.html).", "cell_type": "markdown", "metadata": {}}, {"source": "----------\n## Notebook setup\n\nWhen using PySpark kernel notebooks on HDInsight, there is no need to create a SparkContext or a SparkSession; a SparkSession which has the SparkContext is created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\n- SparkSession (spark)\n\nTo run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**.", "cell_type": "markdown", "metadata": {}}, {"source": "----------\n\n## What is an RDD?\n\nBig Data applications rely on iterative, distributed computing for faster processing of large data sets. To distribute data processing over multiple jobs, the data is typically reused or shared across jobs. To share data between  existing distributed computing systems you need to store data in some intermediate stable distributed store such as HDFS. This makes the overall computations of jobs slower.\n\n**Resilient Distributed Datasets** or RDDs address this by enabling fault-tolerant, distributed, in-memory computations.\n\n----------\n\n## How do I make an RDD?\n\nRDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from the sample data files available in the storage container associated with your Spark cluster. One such sample data file is available on the cluster at `wasb:///example/data/fruits.txt`. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "fruits = spark.sparkContext.textFile('wasb:///example/data/fruits.txt')\nyellowThings = spark.sparkContext.textFile('wasb:///example/data/yellowthings.txt')", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1515736974360_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-dexter.c2whhr1xdp4edkuukzeqbn2e0a.rx.internal.cloudapp.net:8088/proxy/application_1515736974360_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.14:30060/node/containerlogs/container_1515736974360_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"source": "For more examples on how to create RDDs see the following notebooks available with your Spark cluster:\n\n* Read and write data from Azure Storage Blobs (WASB)\n* Read and write data from Hive tables", "cell_type": "markdown", "metadata": {}}, {"source": "----------\n\n## What are RDD operations?\nRDDs support two types of operations: transformations and actions.\n\n* **Transformations** create a new dataset from an existing one. Transformations are lazy, meaning that no transformation is executed until you execute an action.\n* **Actions** return a value to the driver program after running a computation on the dataset.\n\n### RDD transformations\nFollowing are examples of some of the common transformations available. For a detailed list, see [RDD Transformations](https://spark.apache.org/docs/2.0.0/programming-guide.html#transformations)\n\nRun some transformations below to understand this better. Place the cursor in the cell and press **SHIFT + ENTER**.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "yellowThings", "outputs": [{"output_type": "stream", "name": "stdout", "text": "wasb:///example/data/yellowthings.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "# map\nfruitsReversed = fruits.map(lambda fruit: fruit[::-1])", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 4, "cell_type": "code", "source": "fruits.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "8"}], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "# filter\nshortFruits = fruits.filter(lambda fruit: len(fruit) <= 5)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 6, "cell_type": "code", "source": "# flatMap\ncharacters = fruits.flatMap(lambda fruit: list(fruit))", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 7, "cell_type": "code", "source": "# union\nfruitsAndYellowThings = fruits.union(yellowThings)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 8, "cell_type": "code", "source": "# intersection\nyellowFruits = fruits.intersection(yellowThings)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 9, "cell_type": "code", "source": "# distinct\ndistinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()\ndistinctFruitsAndYellowThings", "outputs": [{"output_type": "stream", "name": "stdout", "text": "PythonRDD[17] at RDD at PythonRDD.scala:48"}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "# groupByKey\nyellowThingsByFirstLetter = yellowThings.map(lambda thing: (thing[0], thing)).groupByKey()", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 11, "cell_type": "code", "source": "# reduceByKey\nnumFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda x, y: x + y)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### RDD actions\nFollowing are examples of some of the common actions available. For a detailed list, see [RDD Actions](https://spark.apache.org/docs/2.0.0/programming-guide.html#actions).\n\nRun some transformations below to understand this better. Place the cursor in the cell and press **SHIFT + ENTER**.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "# collect\nfruitsArray = fruits.collect()\nyellowThingsArray = yellowThings.collect()\nfruitsArray", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[u'apple', u'banana', u'canary melon', u'grape', u'lemon', u'orange', u'pineapple', u'strawberry']"}], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "# count\nnumFruits = fruits.count()\nnumFruits", "outputs": [{"output_type": "stream", "name": "stdout", "text": "8"}], "metadata": {"collapsed": false}}, {"execution_count": 14, "cell_type": "code", "source": "# take\nfirst3Fruits = fruits.take(3)\nfirst3Fruits", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[u'apple', u'banana', u'canary melon']"}], "metadata": {"collapsed": false}}, {"execution_count": 15, "cell_type": "code", "source": "# reduce\nletterSet = fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y))\nletterSet", "outputs": [{"output_type": "stream", "name": "stdout", "text": "set([u'a', u' ', u'c', u'b', u'e', u'g', u'i', u'm', u'l', u'o', u'n', u'p', u's', u'r', u't', u'w', u'y'])"}], "metadata": {"collapsed": false}}, {"source": "> **IMPORTANT**: Another important RDD action is saving the output to a file. See the **Read and write data from Azure Storage Blobs (WASB)** notebook for more information.", "cell_type": "markdown", "metadata": {}}, {"source": "----------\n\n## What is a dataframe?\n\nThe `pyspark.sql` library provides an alternative API for manipulating structured datasets, known as \"dataframes\". (Dataframes are not a Spark-specific concept but `pyspark` provides its own dedicated dataframe library.) These are different from RDDs, but you can convert an RDD into a dataframe or vice-versa, if required.\n\nSee [Spark SQL and DataFrame Guide](https://spark.apache.org/docs/2.0.0/sql-programming-guide.html#datasets-and-dataframes) for more information.\n\n### How do I make a dataframe?\n\nYou can load a dataframe directly from an input data source. See the following notebooks included with your Spark cluster for more information.\n\n* Read and write data from Azure Storage Blobs (WASB)\n* Read and write data from Hive tables\n\nYou can also create a dataframe from a CSV file as shown below.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "df = spark.read.csv('wasb:///HdiSamples/HdiSamples/SensorSampleData/building/building.csv', header=True, inferSchema=True)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Dataframe operations\n\nRun the cells below to see examples of some of the the operations that you can perform on dataframes.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "# show the content of the dataframe\ndf.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-----------+-----------+-----------+------------+\n|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n+----------+-----------+-----------+-----------+------------+\n|         1|         M1|         25|     AC1000|         USA|\n|         2|         M2|         27|     FN39TG|      France|\n|         3|         M3|         28|     JDNS77|      Brazil|\n|         4|         M4|         17|     GG1919|     Finland|\n|         5|         M5|          3|    ACMAX22|   Hong Kong|\n|         6|         M6|          9|     AC1000|   Singapore|\n|         7|         M7|         13|     FN39TG|South Africa|\n|         8|         M8|         25|     JDNS77|   Australia|\n|         9|         M9|         11|     GG1919|      Mexico|\n|        10|        M10|         23|    ACMAX22|       China|\n|        11|        M11|         14|     AC1000|     Belgium|\n|        12|        M12|         26|     FN39TG|     Finland|\n|        13|        M13|         25|     JDNS77|Saudi Arabia|\n|        14|        M14|         17|     GG1919|     Germany|\n|        15|        M15|         19|    ACMAX22|      Israel|\n|        16|        M16|         23|     AC1000|      Turkey|\n|        17|        M17|         11|     FN39TG|       Egypt|\n|        18|        M18|         25|     JDNS77|   Indonesia|\n|        19|        M19|         14|     GG1919|      Canada|\n|        20|        M20|         19|    ACMAX22|   Argentina|\n+----------+-----------+-----------+-----------+------------+"}], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "# Print the dataframe schema in a tree format\ndf.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- BuildingID: integer (nullable = true)\n |-- BuildingMgr: string (nullable = true)\n |-- BuildingAge: integer (nullable = true)\n |-- HVACproduct: string (nullable = true)\n |-- Country: string (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": "# Create an RDD from the dataframe\ndfrdd = df.rdd\ndfrdd.take(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(BuildingID=1, BuildingMgr=u'M1', BuildingAge=25, HVACproduct=u'AC1000', Country=u'USA'), Row(BuildingID=2, BuildingMgr=u'M2', BuildingAge=27, HVACproduct=u'FN39TG', Country=u'France'), Row(BuildingID=3, BuildingMgr=u'M3', BuildingAge=28, HVACproduct=u'JDNS77', Country=u'Brazil')]"}], "metadata": {"collapsed": false}}, {"execution_count": 20, "cell_type": "code", "source": "dfrdd = df.rdd\ndfrdd.take(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(BuildingID=1, BuildingMgr=u'M1', BuildingAge=25, HVACproduct=u'AC1000', Country=u'USA'), Row(BuildingID=2, BuildingMgr=u'M2', BuildingAge=27, HVACproduct=u'FN39TG', Country=u'France'), Row(BuildingID=3, BuildingMgr=u'M3', BuildingAge=28, HVACproduct=u'JDNS77', Country=u'Brazil')]"}], "metadata": {"collapsed": false}}, {"execution_count": 21, "cell_type": "code", "source": "# Retrieve a given number of rows from the dataframe\ndf.limit(3).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-----------+-----------+-----------+-------+\n|BuildingID|BuildingMgr|BuildingAge|HVACproduct|Country|\n+----------+-----------+-----------+-----------+-------+\n|         1|         M1|         25|     AC1000|    USA|\n|         2|         M2|         27|     FN39TG| France|\n|         3|         M3|         28|     JDNS77| Brazil|\n+----------+-----------+-----------+-----------+-------+"}], "metadata": {"collapsed": false}}, {"execution_count": 22, "cell_type": "code", "source": "df.limit(3).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-----------+-----------+-----------+-------+\n|BuildingID|BuildingMgr|BuildingAge|HVACproduct|Country|\n+----------+-----------+-----------+-----------+-------+\n|         1|         M1|         25|     AC1000|    USA|\n|         2|         M2|         27|     FN39TG| France|\n|         3|         M3|         28|     JDNS77| Brazil|\n+----------+-----------+-----------+-----------+-------+"}], "metadata": {"collapsed": false}}, {"execution_count": 23, "cell_type": "code", "source": "# Retrieve specific columns from the dataframe\ndf.select('BuildingID', 'Country').limit(3).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-------+\n|BuildingID|Country|\n+----------+-------+\n|         1|    USA|\n|         2| France|\n|         3| Brazil|\n+----------+-------+"}], "metadata": {"collapsed": false}}, {"execution_count": 24, "cell_type": "code", "source": "# Use GroupBy clause with dataframe \ndf.groupBy('HVACProduct').count().select('HVACProduct', 'count').show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+-----+\n|HVACProduct|count|\n+-----------+-----+\n|    ACMAX22|    4|\n|     AC1000|    4|\n|     JDNS77|    4|\n|     FN39TG|    4|\n|     GG1919|    4|\n+-----------+-----+"}], "metadata": {"collapsed": false}}, {"source": "> **IMPORTANT**: Many of the methods available on normal RDDs are also available on dataframes. For example, `distinct`, `count`, `collect`, `filter`, `map`, and `take` are all methods on dataframes as well as on RDDs.", "cell_type": "markdown", "metadata": {}}, {"source": "-------\n\n## Spark SQL and dataframes\n\nYou can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession. Run the snippet below to see an example.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "# Register the dataframe as a temporary table called HVAC\ndf.registerTempTable('HVAC')", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 26, "cell_type": "code", "source": "%%sql\nSELECT * FROM HVAC WHERE BuildingAge >= 10", "outputs": [{"ename": "AttributeError", "evalue": "'module' object has no attribute 'api'", "traceback": ["\u001b[1;31m\u001b[0m", "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/IPython/core/formatters.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    902\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36mdisplay_dataframe\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdisplay_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mselected_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mselected_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     encoding = Encoding(chart_type=Encoding.chart_type_table, x=selected_x, y=selected_y,\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36mselect_x\u001b[1;34m(data, order)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0m_validate_custom_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_classify_data_by_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mchosen_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36m_classify_data_by_type\u001b[1;34m(data, order, skip)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mskip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfer_vegalite_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36minfer_vegalite_type\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \"\"\"\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     if typ in ['floating', 'mixed-integer-float', 'integer',\n", "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'api'"], "output_type": "error"}, {"execution_count": 26, "output_type": "execute_result", "data": {"text/plain": "    BuildingID BuildingMgr  BuildingAge HVACproduct       Country\n0            1          M1           25      AC1000           USA\n1            2          M2           27      FN39TG        France\n2            3          M3           28      JDNS77        Brazil\n3            4          M4           17      GG1919       Finland\n4            7          M7           13      FN39TG  South Africa\n5            8          M8           25      JDNS77     Australia\n6            9          M9           11      GG1919        Mexico\n7           10         M10           23     ACMAX22         China\n8           11         M11           14      AC1000       Belgium\n9           12         M12           26      FN39TG       Finland\n10          13         M13           25      JDNS77  Saudi Arabia\n11          14         M14           17      GG1919       Germany\n12          15         M15           19     ACMAX22        Israel\n13          16         M16           23      AC1000        Turkey\n14          17         M17           11      FN39TG         Egypt\n15          18         M18           25      JDNS77     Indonesia\n16          19         M19           14      GG1919        Canada\n17          20         M20           19     ACMAX22     Argentina", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BuildingID</th>\n      <th>BuildingMgr</th>\n      <th>BuildingAge</th>\n      <th>HVACproduct</th>\n      <th>Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>M1</td>\n      <td>25</td>\n      <td>AC1000</td>\n      <td>USA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>M2</td>\n      <td>27</td>\n      <td>FN39TG</td>\n      <td>France</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>M3</td>\n      <td>28</td>\n      <td>JDNS77</td>\n      <td>Brazil</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>M4</td>\n      <td>17</td>\n      <td>GG1919</td>\n      <td>Finland</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>M7</td>\n      <td>13</td>\n      <td>FN39TG</td>\n      <td>South Africa</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>M8</td>\n      <td>25</td>\n      <td>JDNS77</td>\n      <td>Australia</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>9</td>\n      <td>M9</td>\n      <td>11</td>\n      <td>GG1919</td>\n      <td>Mexico</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>M10</td>\n      <td>23</td>\n      <td>ACMAX22</td>\n      <td>China</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>11</td>\n      <td>M11</td>\n      <td>14</td>\n      <td>AC1000</td>\n      <td>Belgium</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>12</td>\n      <td>M12</td>\n      <td>26</td>\n      <td>FN39TG</td>\n      <td>Finland</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>13</td>\n      <td>M13</td>\n      <td>25</td>\n      <td>JDNS77</td>\n      <td>Saudi Arabia</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>14</td>\n      <td>M14</td>\n      <td>17</td>\n      <td>GG1919</td>\n      <td>Germany</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>15</td>\n      <td>M15</td>\n      <td>19</td>\n      <td>ACMAX22</td>\n      <td>Israel</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>16</td>\n      <td>M16</td>\n      <td>23</td>\n      <td>AC1000</td>\n      <td>Turkey</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>17</td>\n      <td>M17</td>\n      <td>11</td>\n      <td>FN39TG</td>\n      <td>Egypt</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>18</td>\n      <td>M18</td>\n      <td>25</td>\n      <td>JDNS77</td>\n      <td>Indonesia</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>19</td>\n      <td>M19</td>\n      <td>14</td>\n      <td>GG1919</td>\n      <td>Canada</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>20</td>\n      <td>M20</td>\n      <td>19</td>\n      <td>ACMAX22</td>\n      <td>Argentina</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 27, "cell_type": "code", "source": "%%sql \nSELECT BuildingID, Country FROM HVAC LIMIT 3", "outputs": [{"ename": "AttributeError", "evalue": "'module' object has no attribute 'api'", "traceback": ["\u001b[1;31m\u001b[0m", "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/IPython/core/formatters.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    902\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36mdisplay_dataframe\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdisplay_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mselected_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mselected_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     encoding = Encoding(chart_type=Encoding.chart_type_table, x=selected_x, y=selected_y,\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36mselect_x\u001b[1;34m(data, order)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0m_validate_custom_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_classify_data_by_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mchosen_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36m_classify_data_by_type\u001b[1;34m(data, order, skip)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mskip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfer_vegalite_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36minfer_vegalite_type\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \"\"\"\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     if typ in ['floating', 'mixed-integer-float', 'integer',\n", "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'api'"], "output_type": "error"}, {"execution_count": 27, "output_type": "execute_result", "data": {"text/plain": "   BuildingID Country\n0           1     USA\n1           2  France\n2           3  Brazil", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BuildingID</th>\n      <th>Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>USA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>France</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Brazil</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "widgets": {"state": {"44e7033919664080af75bf31b15e096c": {"views": [{"cell_index": 35}]}, "3748bc4e9bf74080a8f821881f2668c1": {"views": [{"cell_index": 36}]}}, "version": "1.2.0"}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}